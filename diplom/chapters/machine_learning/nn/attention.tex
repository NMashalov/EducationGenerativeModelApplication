Модель Transformer является архитектурой глубокого обучения, предназначенной для обработки последовательных данных,
таких как тексты или временные ряды. 
Она была предложена в работе \cite{vaswani2017attention} и стала одной из самых инновационных архитектур
в области обработки естественного языка.

Основной компонент модели Transformer это механизм внимания. Он позволяет модели сосредоточиться 
на наиболее важных частях входных данных при выполнении задач, таких как машинный перевод или обработка текста.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/ml/nn/transformer.excalidraw.png}
    \caption{Механизм внимания в архитектуре Transformer \cite{vaswani2017attention} }
    \label{attention}
\end{figure}

Механизм внимания в Transformer состоит из трех основных частей:\begin{enumerate}
    \item  Расчет векторов запроса, ключа и значения.
    Они используются для вычисления весов входных данных и определения их важности для каждого элемента:
    \begin{equation}
        \begin{aligned}
            &q =W_q x \\ 
            &k = W_k x \\
            &v = W_v x,
        \end{aligned}
    \end{equation}
    , где \(W_q\), \(W_k\), \(W_v\) - матрицы весов, обучаемые моделью.
    \item Вычисление векторов запроса и ключа, для каждого элемента \(x_i\). Рассчитываются логиты \(e_{ij}\):
    \begin{equation}
        e_{ij} = \frac{q \cdot k_j}{\sqrt{d_k}} ,
    \end{equation}
    где \(d_k\) - длина запроса,
    \item преобразуются в веса внимания \( \alpha_{ij} \) с помощью функции softmax:
    \begin{equation}
        \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{j'} \exp(e_{ij'})}.
    \end{equation}
\end{enumerate}

Веса определяют важность каждого элемента данных, которую модель учитывает при решении конкретной задачи. 
После вычисления весов внимания, они умножаются на соответствующие значения (value) и суммируются, 
чтобы получить итоговый взвешенный вектор, который представляет собой выходной результат механизма внимания.