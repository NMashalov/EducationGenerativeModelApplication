\texit{Определение} \textbf{Нейронные сети} представляют собой вычислительные модели, состоящие из узлов, называемых нейронами, организованных в слои.
Каждый нейрон взвешивает входные сигналы,  представленные как вектор \( \mathbf{x} = (x_1, x_2, ..., x_n) \), 
 с весами $2$\mathbf{w} = (w_1, w_2, ..., w_n) \) и смещением \( b \), 
 где \( n \) - количество входов, \( x_i \) - \( i \)-й входной сигнал,
  \( w_i \) - весовой коэффициент \( i \)-го входа, \( b \) - смещение (bias). 
  


\textit{Определение} \textbf{Функцией активации} в нейронной сети называется \textit{нелинейная} функция.

Классическими примерами функции активации являются сигмоида, ReLU \cite{agarap2018deep} и GeLU \cite{hendrycks2016gaussian}

\begin{multline}
  &ReLU(x) = \min(0,x)p \\
  &
$$


Функция активации обычно вводится для добавления нелинейности в модель, что позволяет нейронной сети моделировать сложные нелинейные зависимости в данных.
Некоторые из распространенных функций активации включают в себя сигмоидальную функцию (\( \sigma \)), гиперболический тангенс (\( \tanh \)), ReLU (Rectified Linear Unit) и их вариации.

В случае многослойной нейронной сети, выходы нейронов одного слоя становятся входами для следующего слоя, образуя цепочку преобразований. Процесс передачи данных через нейроны последовательных слоев называется прямым распространением (forward propagation).

Нейронные сети обучаются путем настройки весов \( \mathbf{w} \) и смещений \( b \) с использованием алгоритмов оптимизации, таких как градиентный спуск. Во время обучения модель минимизирует функцию потерь \( L \), которая оценивает разницу между предсказанным результатом и истинным значением:

\[ L = \frac{1}{N} \sum_{i=1}^{N} L(y_i, \hat{y}_i) \]

где \( N \) - количество обучающих примеров, \( y_i \) - истинное значение, \( \hat{y}_i \) - предсказанное значение.