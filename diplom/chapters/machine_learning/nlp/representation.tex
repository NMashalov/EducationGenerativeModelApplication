\textit{Определение:} \textbf{N-граммы} представляют собой последовательности из \( n \) элементов в тексте или последовательности символов, 
где \( n \) обозначает количество элементов в последовательности. Элементы могут быть символами, словами или более крупными фрагментами текста 
в зависимости от контекста применения. Анализ n-грамм является важным методом в обработке естественного языка (Natural Language Processing, NLP) 
для изучения частотности последовательностей слов или символов в текстовых данных.

Формально, n-грамма \( \text{ngram}_n \) длины \( n \) в тексте \( T \) определяется как последовательность \( n \) элементов, 
где каждый элемент \( x_i \) может быть символом, словом или другими составляющими текста:

\[ \text{ngram}_n = (x_1, x_2, ..., x_n) \]

Использование n-грамм в анализе текста позволяет оценивать частотность последовательностей слов или символов и анализировать лингвистические характеристики
 текста, такие как структура, стиль и тематика. Кроме того, n-граммы могут использоваться в задачах моделирования языка, предсказания следующего слова в предложении, 
 а также в машинном переводе и других приложениях обработки естественного языка.

\textit{Определение:} \textbf{Токенизация} --- процесс в котором текст разбивается на токены. 
Это позволяет применить лемматизацию к каждому слову в тексте независимо от контекста.
Лемматизация часто используется в различных областях NLP, включая информационный поиск, анализ тональности, машинный перевод и другие.

\textit{Определение:} \textbf{Векторное вложение}(\textit{англ} embedding) --- векторное представление $\mathbb{R}^N$ слова $w$,
проявляющее семантические и синтаксические свойства при операциях сложения и взятия косинусного расстояния.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/ml/nlp/word2vec.excalidraw.png}
    \caption{Векторное вложение позволяет выполнять семантические операции}
    \label{word2vec}
\end{figure}

Практически востребованной оказалась дистрибутивная гипотеза \cite{harris1954distributional}, легшая в основу алгоритма Word2Vec 
\cite{mikolov2013efficient}:
\begin{equation}
    \cos(\theta) = \frac{w_1 \cdot w_2}{\|w_1\| \|w_2\|}.
\end{equation}
Векторные вложения слов играют важную роль в генеративном моделировании естественного языка, так как они позволяют моделям представлять слова в виде непрерывных 
числовых значений, которые могут быть использованы как входные данные для алгоритмов машинного обучения. Это позволяет моделям эффективно сопоставлять зависимости 
между словами и генерировать семантически богатые и лингвистически осмысленные тексты. Известны и другие подходы GloVe \cite{pennington2014glove}, FastText \cite{bojanowski2017enriching},
улучшающие базовый подход работой с разбиением слова на части.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/ml/nlp/vector.excalidraw.png}
    \caption{Векторное представление позволяет выполнять семантические операции сложения и вычитания}
    \label{embedding}
\end{figure}
Появление трансформерной архитектуры нейросетей позволило улучшить результаты семантического представления \cite{devlin2018bert}. 
Для сравнения стали доступны не только слова, но целые предложения и параграфы. Результат был достигнут путем специального подхода к обучению,
заключающегося в случайном исключении до 20\% слов во фрагменте текста, который требуется восстановить. Сложность задачи 
заключается в обширном окне контекста, необходимого для учета, при заполнении пропусков. Помимо восстановления слов, модель BERT
также обучалась на задачах классификации и выделения ключевых слов в предложении. Совмещение техник обучения позволяет
улучшить метрические показатели моделей до нескольких десятков процентных пунктов \cite{hospedales2021meta}.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{assets/ml/nlp/bert.excalidraw.png}
%     \caption{Векторное позволяет выполнять семантические операции}
%     \label{nert}
% \end{figure}







