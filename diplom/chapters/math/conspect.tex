Задача оптимизации  в общем случае записывается как \cite{nesterov2015universal}

\begin{equation}
    f(x) = \mathrm{E} f(x,\xi) \rightarrow \min_x,
\end{equation}
где двойка $(x,\xi)$ является результатом наблюдения.


Пусть $x^*$ - оптимальный параметр.

\texit{Определение} Стохастический градиент 

\begin{equation}
    \nabla f(x,\xi) = x - 
\end{equation}

Если $f(x)$ - $\mu$ сильно выпукла и $L$ -k

Пусть f - 

\begin{equation}
    \nabla^2 f(x) = \frac{\partial^2 f}{\partial x_i \partial x_j} 
\end{equation}

В заданных 


Мажорирует гессиан функции $f$

\begin{equation}
    \mu I \prec \nabla^2 f(x) \prec L I 
\end{equation}

Алтернативная формулировка. Пусть $\lambda_1, \dots, \lambda_n$ - упорядоченные собственные числа матрицы Гессиана. Для удобства обозначим
$\lambda_{min} = \lambda_1$ и $\lambda_{max} = \lambda_n$:

\begin{equation}
    \lambda_{min} \nabla^2 f(x) \ge \mu; \\
    \lambda_{max} \nabla^2 f(x) \le L;
\end{equation}

Зададим схему обновления 

\begin{equation}
    x^{k+1} = x^k - h_k \nabla f(x^k, \xi^K)
\end{equation}

\textit{Определение} \textbf{Усредненным по подвыборке} мощностью $B$ стохастическим градиентом называется 
$\nabla^B f(x,\xi) = \sum_{j=i}^B \nabla f(x,\xi_i)$.

Уточним, что усреднение под выборки выполняется в одной точке $x$.

Современные достижения в направлении получены авторами Srebro, Woodworth \cite{NEURIPS2021_3c63ec7b} \cite{woodworth2016tight}

\texit{Теорема} Пусть функция 


Обозначим за $\Delta f = f(x_0) - f(x^*)$ начальную невязку. 

\begin{equation}
    \mathrm{E} f(x^N) -f(x^*) \le 
    \delta f \exp\left(-c1 \sqrt{\frac{M}{L}} N \right) + 
    \delta f \exp\left(\right) exp(-c_2 \frac{M}{L} b_N) +
    \frac{\sigma^2_*}{\mu b_N},
\end{equation}
где $\sigma^2_* = \mathrm{E} \|\nabla f(x_0,\xi) \|^2$

Полученный результат говорит об оптимальном размере подвыборки для спуска.
\begin{equation}
    \mathrm{E} \frac{\mu}{2} \| x^N - x_0 \|_2^2 \le f(x^N) - f(x_0))\ke 
\end{equation}

Устойчивость метода при выходе на плато
Схема Поляка-Руперта-Юдицкого
\begin{equation}
    x^{k+1} = x^{k} - h_k \
\end{equation}
Шаги $h_k \sim k^{-\alpha}$, $\alpha in (\frac{1}{2},1)$.
При этом ошибка считается для среднего
$$
    \bar{x_n} = \frac{1}{N} \sum_{k=1} x^k
$$
\begin{equation}
    \Sigma = \mathm{E}\left[\nabla f(x_*,\xi)\nabla f(x_*, \xi)^T\right]
\end{equation}
Для этого случая имеет сходимость по распределению.
\begin{equation}
    (\hat{x}^N - x_*) \rightarrow
\end{equation}
Минимизация функции риска
$$
    \hat{x}_{CRM} = \argmin_x \frac{1}{N} f(x,\xi^k)
$$
\cite{duchi2021asymptotic}