Для случая, в котором 
Стохастическая аппроксимация 


\textit{Определение} \textbf{Стохастическая аппроксимация} - метод решения задач статистического оценивания,
 строящихся в виде последовательного приближения на основании наблюдений, представленных случайной величиной.

Стохастическая аппроксимации

$$
    P(\mathbf{T}| \mathbf{X},\mathbf{\Theta})
$$

Наиболее известным результатом является Алгоритм Роббинса-Монро

\textit{\textbf{Теорема.} Алгоритм Роббинса-Монро}. Алгоритм, выполняющий пересчет по правилу $d_{n+1} =d _n + a_d(s^*-s)$ сходится в $L^2$ норме при выполнении \begin{enumerate}
    \item значения функции отклика монотонны и $s$ ограничены: $\exists N\ \forall x : | s(x) | \le N$
    \item $\sum_{t=0}^\infty a_t = \infty$
    \item $\exists M : |\sum_{t=0}^\infty a_t^2 |< M$.
\end{enumerate}

\textit{Доказательство теоремы Роббинса-Монро} \label{monro}
Следуя доказательству \cite{blum1954approximation}, используем рекуррентную схема связи между ошибками на каждом шаге алгоритма $b_1,\dots,b_n$. 
В этом случае правило обновления Роббинса-Монро запишется как: 
\begin{equation}
    \mathrm{E_{x_t \sim p(x|s(d))}}(d_n+a_t(s^*-x_t) -d^*)^2.
\end{equation}
Раскрываем квадрат разности:
\begin{equation}
    (d_n - d^*)^2 + a_t^2 \mathrm{E_{x_t \sim p(x|s(d))}} (s^*-x_t)^2 - 2 a_t \mathrm{E_{x_t \sim p(x|s(d))}}\left[ (s^*-x_t)(d_n-d^*) \right].
\end{equation}
Используем несмещенность оценки $\mathrm{E}_{x \sim p(x|s)} x = s$:
\begin{equation}
    d_n =2 a_t (s^* -s(d))(d_n-d^*).
\end{equation}
Положительна, исходя из монотонности:
\begin{equation}
    \lim_{n \rightarrow \infty} b_n = b_1 + \sum_{j=1}^\infty a_n^2 c_n -2 \sum_1^{\infty} a_n d_n < \infty.
\end{equation}
Сходимость обоснуем через два последовательных шага: \begin{itemize}
    \item  $\sum_{j=1}^\infty a_n^2 c_n < \infty$ , поскольку при $\sum_{j=1} a_n^2 < \infty $ и $c_n$ -ограничены.
    \item $\sum_{j=1}^\infty a_n d_n < \sum_{j=1}^\infty a_n^2 c_n$,  т.к $b_n \ge 0$.
\end{itemize}
Покажем, что $\lim_{n \rightarrow \infty}{b_n}$  нулю следует из$\sum_{j=1}^\infty a_j  =\infty$ .
Действительно, зададим ряд $m_n = \frac{b_n}{d_n}$, т.к. он неотрицателен, то $\sum_{i=1}^{\infty} a_n k_n = \infty $. 
Тогда $\sum_{.i=1}^{\infty} a_n k_n b_N < \infty $. 
Из сходимости ряда из неотрицательных элементов следует, что $\lim_{n \rightarrow \infty}b_n =0$.
$\blacksquare$


Если функция отклика $s$ строго выпукла и дважды дифференцируема,
то асимптотическая скорость сходимости равна $\mathcal{O}(\frac{1}{n})$ \cite{sacks1958asymptotic}.

Выбор коэффициент $a_n$ существенно образом влияет на число шагом сходимости последовательности.
Авторы оригинального алгоритма предлагают $a_n = \frac{\lambda}{n}$. В работах \cite{lai1979adaptive}
используется альтернативный подход, исходящий из \begin{enumerate}
    \item несмещенности оценки $\mathrm{E}d_n=d$.
    \item минимизации дисперсии $\mathbf{D} d_n=0$.
\end{enumerate}
Современный подход направлен на учетом априорного представления в виде нормального распределения \begin{enumerate}
    \item нормальным распределением \cite{joseph2004efficient}
    \item многомерных биноминальных распределений \cite{xiong2018efficient}
    \item цепи гауссовых распределений \cite{liu2024robbins}.
\end{enumerate}
Подвергается изменениям и сама схема доказательства как в работе \cite{красулина2007односторонней}.
Автор предлагает заменить оптимизируемый коэффициент $s^*$ на параметр $b_n$:

\textit{Случай $b_n \ne s^*$} \label{ino}
Общая схема доказательства приведена в \cite{lai1979adaptive}.
$\blacksquare$

 
\textit{Теоерма}\cite{xiong2018efficient}
 Пусть $\vec{x}$ - вектор бернулевских случайных величин с параметрами $\vec{s}=f(\vec{d})$, где $f(\vec{x})$ - выпуклая.
 Тогда cхема пересчета $d_t=\vec{x}_t + A^{(t)} (s - \vec{x}_t)$ с шагами $A^{(t)}$ удовлетворяющих условиям:
 $\forall t,j \rightarrow a_{jj}^{(t)} >0,
 \sum^{\infty}_{t=1} a_{jj}^{(t)} = \infty,
  \sum_{n=1}^\infty (a^{(t)}_{jj})^2 < \infty$
 сходится по вероятности к целевому значению $\vec{s}^*$

Существенный вклад в развитие методов стохастической аппроксимации внёс Борис Теодорович Поляк \cite{polyak1990new},
предложивший метод усреднения управляющего параметра $s$:
 \begin{equation}
     \hat{s}_{n+1} = \sum_{i=0}^n s_i.
     \label{polyak}
 \end{equation}
Такой подход позволяет подавлять высокочастотные шумовые компоненты при аппроксимации ряда, что позволяет использовать методы с большим шагом спуска.
 Условия эффективной применимости состоят в малом изменении коэффициентов $a_n$:
 \begin{equation}
    \frac{a_{n}-a_{n+1}}{a_{n}}=\mathit{o}(a_{n}).
    \label{polyak_assumptions}
 \end{equation}

 Схема Поляка-Руперта-Юдицкого. Стохастической аппроксимации,
изложенному в работе Герберта Робинсона и Суттона Монро \cite{robbins1951stochastic}




\begin{equation}
    x^{k+1} = x^{k} - \gamma_k \phi(\nabla_x f(x^k,\xi^k))
\end{equation}
Шаги $h_k \sim k^{-\alpha}$, $\alpha in (\frac{1}{2},1)$.
При этом ошибка считается для среднего
\begin{equation}
    \bar{x_n} = \frac{1}{N} \sum_{k=1} x^k
\end{equation}

